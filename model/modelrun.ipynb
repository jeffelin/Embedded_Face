{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Running the Model\n",
    "\n",
    "From Shiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidParameterError",
     "evalue": "The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidParameterError\u001b[0m                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 81\u001b[0m\n\u001b[0;32m     78\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m target_data[train_index], target_data[test_index]\n\u001b[0;32m     80\u001b[0m classifier \u001b[38;5;241m=\u001b[39m RandomForestClassifier(max_depth \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m6\u001b[39m, min_samples_leaf \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m3\u001b[39m, min_samples_split \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m, class_weight \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m, bootstrap \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m, max_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauto\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m100\u001b[39m) \u001b[38;5;66;03m# generates random forest model\u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m classifier\u001b[38;5;241m.\u001b[39mfit(x_train, y_train) \u001b[38;5;66;03m# trains the model using the training sets\u001b[39;00m\n\u001b[0;32m     82\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(x_test) \u001b[38;5;66;03m# predicts the data for the test dataset\u001b[39;00m\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(y_pred)):\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\sklearn\\base.py:1467\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1462\u001b[0m partial_fit_and_fitted \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m   1463\u001b[0m     fit_method\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpartial_fit\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m _is_fitted(estimator)\n\u001b[0;32m   1464\u001b[0m )\n\u001b[0;32m   1466\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m global_skip_validation \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m partial_fit_and_fitted:\n\u001b[1;32m-> 1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[0;32m   1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\sklearn\\base.py:666\u001b[0m, in \u001b[0;36mBaseEstimator._validate_params\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    658\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_params\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    659\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate types and values of constructor parameters\u001b[39;00m\n\u001b[0;32m    660\u001b[0m \n\u001b[0;32m    661\u001b[0m \u001b[38;5;124;03m    The expected type and values must be defined in the `_parameter_constraints`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    664\u001b[0m \u001b[38;5;124;03m    accepted constraints.\u001b[39;00m\n\u001b[0;32m    665\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 666\u001b[0m     validate_parameter_constraints(\n\u001b[0;32m    667\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_parameter_constraints,\n\u001b[0;32m    668\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_params(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m    669\u001b[0m         caller_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m,\n\u001b[0;32m    670\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:95\u001b[0m, in \u001b[0;36mvalidate_parameter_constraints\u001b[1;34m(parameter_constraints, params, caller_name)\u001b[0m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     90\u001b[0m     constraints_str \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     91\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mstr\u001b[39m(c)\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39mconstraints[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m or\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     92\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     93\u001b[0m     )\n\u001b[1;32m---> 95\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m InvalidParameterError(\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_name\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m parameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcaller_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     97\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconstraints_str\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_val\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     98\u001b[0m )\n",
      "\u001b[1;31mInvalidParameterError\u001b[0m: The 'max_features' parameter of RandomForestClassifier must be an int in the range [1, inf), a float in the range (0.0, 1.0], a str among {'log2', 'sqrt'} or None. Got 'auto' instead."
     ]
    }
   ],
   "source": [
    "#Face Model\n",
    "import csv # for reading the csv file\n",
    "import pandas as pd # uses dataframes from pandas\n",
    "import numpy as np # used for arrays\n",
    "from sklearn.ensemble import RandomForestClassifier # used for creating FSS score predictors --> Random Forest algorithm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold # for implementing k-folds\n",
    "import matplotlib.pyplot as plt # used for plotting the graphs\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "# FUNCTIONS:\n",
    "\n",
    "def create_conf_matrix(y_preds, y_reals):\n",
    "\tTP = 0\n",
    "\tFN = 0\n",
    "\tFP = 0\n",
    "\tTN = 0\n",
    "\tfalsePos = []\n",
    "\tfalseNeg = []\n",
    "\n",
    "\tfor i in range(len(y_preds)):\n",
    "\t\tpred_val = y_preds[i]\n",
    "\t\ttest_val = y_reals[i]\n",
    "\n",
    "\t\tif test_val == 1 and pred_val == 1:\n",
    "\t\t\tTP += 1\n",
    "\t\tif test_val == 1 and pred_val == 0:\n",
    "\t\t\tFN += 1\n",
    "\t\t\tfalseNeg.append(files[i])\n",
    "\t\tif test_val == 0 and pred_val == 1:\n",
    "\t\t\tFP += 1\n",
    "\t\t\tfalsePos.append(files[i])\n",
    "\t\tif test_val == 0 and pred_val == 0:\n",
    "\t\t\tTN += 1\n",
    "\n",
    "\tfalsePos.sort()\n",
    "\tfalseNeg.sort()\n",
    "\treturn [TP, FN, FP, TN]\n",
    "\n",
    "# calculates accuracy of model results\n",
    "def accuracy(matrix):\n",
    "\tTP = matrix[0]\n",
    "\tTN = matrix[3]\n",
    "\tacc = (TP + TN) / (matrix[0] + matrix[1] + matrix[2] + matrix[3])\n",
    "\treturn acc\n",
    "\n",
    "# calculates the sensitivity of model results\n",
    "def sensitivity(matrix):\n",
    "\tTP = matrix[0]\n",
    "\tFN = matrix[1]\n",
    "\tsens = TP / (TP + FN)\n",
    "\treturn sens\n",
    "\n",
    "# calculates the specificity of model results\n",
    "def specificity(matrix):\n",
    "\tTN = matrix[3]\n",
    "\tFP = matrix[2]\n",
    "\tspec = TN / (TN + FP)\n",
    "\treturn spec\n",
    "\n",
    "\n",
    "\n",
    "dataset = np.array(pd.read_csv(r'C:\\Users\\jeffe\\Downloads\\face_det_hardware\\HumanDetection-Kinect-Mmwave\\model\\ThermalOcc.csv')) # dataset of confidence levels\n",
    "target_data = dataset[:, 5] # UPDATE!\n",
    "num_entries = len(dataset)\n",
    "\n",
    "labels = [\"ID\", \"CONF\"] # UPDATE COLUMN TITLES\n",
    "classes = [\"0\", \"1\"]\n",
    "\n",
    "kfold = KFold(n_splits = 10, shuffle = True, random_state = 100) # creates 10-fold splits\n",
    "face_preds = []\n",
    "face_truths = []\n",
    "\n",
    "for train_index, test_index in kfold.split(dataset):\n",
    "\n",
    "\tx_train, x_test = dataset[train_index], dataset[test_index]\n",
    "\ty_train, y_test = target_data[train_index], target_data[test_index]\n",
    "\n",
    "\tclassifier = RandomForestClassifier(max_depth = 6, min_samples_leaf = 3, min_samples_split = 10, class_weight = 'balanced', bootstrap = False, max_features = \"auto\", random_state = 100) # generates random forest model\n",
    "\tclassifier.fit(x_train, y_train) # trains the model using the training sets\n",
    "\ty_pred = classifier.predict(x_test) # predicts the data for the test dataset\n",
    "\t\n",
    "\tfor i in range(len(y_pred)):\n",
    "\t\tface_preds.append(y_pred[i])\n",
    "\t\tface_truths.append(y_test[i])\n",
    "\n",
    "\n",
    "matrix = create_conf_matrix(face_preds, face_truths, filePreds)\n",
    "print(\"[TP, FN, FP, TN] = \", end = \"\")\n",
    "print(matrix)\n",
    "print(\"\\nAccuracy: \" + str(accuracy(matrix)))\n",
    "print(\"Sensitivity: \" + str(sensitivity(matrix)))\n",
    "print(\"Specificity: \" + str(specificity(matrix)) + \"\\n\")\n",
    "\n",
    "# fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (35, 10), dpi = 400)\n",
    "# tree.plot_tree(classifier.estimators_[0], feature_names = labels, class_names = classes, filled = True, impurity = False, precision = 1, fontsize = 10)\n",
    "# fig.savefig('decisionTree.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'modelData.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 75\u001b[0m\n\u001b[0;32m     70\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m spec\n\u001b[0;32m     73\u001b[0m \u001b[38;5;66;03m# MAIN:\u001b[39;00m\n\u001b[1;32m---> 75\u001b[0m dataset \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodelData.csv\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m     76\u001b[0m target_data \u001b[38;5;241m=\u001b[39m dataset[:, \u001b[38;5;241m5\u001b[39m] \u001b[38;5;66;03m# UPDATE\u001b[39;00m\n\u001b[0;32m     77\u001b[0m num_entries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(dataset)\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'modelData.csv'"
     ]
    }
   ],
   "source": [
    "\n",
    "import csv # for reading the csv file\n",
    "import sys # for max integer num in forward stepwise feature selection\n",
    "import pandas as pd # uses dataframes from pandas\n",
    "import numpy as np # used for arrays\n",
    "from sklearn.ensemble import RandomForestClassifier #  for random forests\n",
    "from sklearn import svm # used for creating mortality models --> Support Vector Machine algorithm\n",
    "from sklearn.neighbors import KNeighborsClassifier # for KNN algorithm\n",
    "from sklearn.linear_model import LogisticRegression # for logistic regression algorithm\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn import metrics # for accuracy calculation\n",
    "from sklearn.ensemble import RandomForestClassifier # used for creating FSS score predictors --> Random Forest algorithm\n",
    "from sklearn.preprocessing import LabelEncoder # for converting the strs to ints\n",
    "import statistics # used for calculating standard deviation\n",
    "from sklearn.metrics import hinge_loss # used to calculating the hinge loss\n",
    "from sklearn.model_selection import KFold # for implementing k-folds\n",
    "import matplotlib.pyplot as plt # used for plotting the graphs\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import MinMaxScaler # used to allow SVM to run faster\n",
    "\n",
    "\n",
    "# FUNCTIONS:\n",
    "\n",
    "# creates the confusion matrix array by calculating the true and false positives and negatives\n",
    "def create_conf_matrix(y_preds, y_reals):\n",
    "\tTP = 0\n",
    "\tFN = 0\n",
    "\tFP = 0\n",
    "\tTN = 0\n",
    "\tfalsePos = []\n",
    "\tfalseNeg = []\n",
    "\n",
    "\tfor i in range(len(y_preds)):\n",
    "\t\tpred_val = y_preds[i]\n",
    "\t\ttest_val = y_reals[i]\n",
    "\n",
    "\t\tif test_val == 1 and pred_val == 1:\n",
    "\t\t\tTP += 1\n",
    "\t\tif test_val == 1 and pred_val == 0:\n",
    "\t\t\tFN += 1\n",
    "\t\t\tfalseNeg.append(i + 1)\n",
    "\t\tif test_val == 0 and pred_val == 1:\n",
    "\t\t\tFP += 1\n",
    "\t\t\tfalsePos.append(i + 1)\n",
    "\t\tif test_val == 0 and pred_val == 0:\n",
    "\t\t\tTN += 1\n",
    "\n",
    "\t# print(falsePos)\n",
    "\t# print(falseNeg)\n",
    "\treturn [TP, FN, FP, TN]\n",
    "\n",
    "# calculates the accuracy of mortality model results\n",
    "def accuracy(matrix):\n",
    "\tTP = matrix[0]\n",
    "\tTN = matrix[3]\n",
    "\tacc = (TP + TN) / (matrix[0] + matrix[1] + matrix[2] + matrix[3])\n",
    "\treturn acc\n",
    "\n",
    "# calculates the sensitivity of mortality model results\n",
    "def sensitivity(matrix):\n",
    "\tTP = matrix[0]\n",
    "\tFN = matrix[1]\n",
    "\tsens = TP / (TP + FN)\n",
    "\treturn sens\n",
    "\n",
    "# calculates the specificity of mortality model results\n",
    "def specificity(matrix):\n",
    "\tTN = matrix[3]\n",
    "\tFP = matrix[2]\n",
    "\tspec = TN / (TN + FP)\n",
    "\treturn spec\n",
    "\n",
    "\n",
    "# MAIN:\n",
    "\n",
    "dataset = np.array(pd.read_csv('modelData.csv'))\n",
    "target_data = dataset[:, 5] # UPDATE\n",
    "num_entries = len(dataset)\n",
    "\n",
    "labels = [\"ID\"]\n",
    "classes = [\"0\", \"1\"]\n",
    "\n",
    "\n",
    "kfold = KFold(n_splits = 10, shuffle = True, random_state = 100) # creates 10-fold splits\n",
    "\n",
    "final_truths = []\n",
    "final_preds1 = []\n",
    "final_preds2 = []\n",
    "final_preds3 = []\n",
    "final_preds4 = []\n",
    "# final_preds5 = []\n",
    "\n",
    "proba_preds1 = []\n",
    "proba_preds2 = []\n",
    "proba_preds3 = []\n",
    "proba_preds4 = []\n",
    "# proba_preds5 = []\n",
    "\n",
    "for train_index, test_index in kfold.split(dataset):\n",
    "\tx_train, x_test = dataset[train_index], dataset[test_index]\n",
    "\ty_train, y_test = target_data[train_index], target_data[test_index]\n",
    "\n",
    "\n",
    "\tclassifier1 = RandomForestClassifier(max_depth = 6, min_samples_leaf = 3, min_samples_split = 10, class_weight = 'balanced', bootstrap = False, max_features = \"auto\", random_state = 100)\n",
    "\tclassifier2 = svm.SVC(kernel = 'linear', random_state = 100)\n",
    "\tclassifier2Prob = svm.SVC(kernel = 'linear', random_state = 100, probability = True)\n",
    "\tclassifier3 = LogisticRegression(random_state = 100)\n",
    "\tclassifier4 = KNeighborsClassifier(n_neighbors = 2, weights = 'distance')\n",
    "\t# classifier5 = MLPClassifier(random_state = 100)\n",
    "\tprint(\"Models Created . . .\")\n",
    "\n",
    "\tscaling = MinMaxScaler(feature_range = (-1,1)).fit(x_train)\n",
    "\tx_trainSVM = scaling.transform(x_train)\n",
    "\tx_testSVM = scaling.transform(x_test)\n",
    "\n",
    "\tclassifier1.fit(x_train, y_train.ravel())\n",
    "\tclassifier2.fit(x_trainSVM, y_train.ravel())\n",
    "\tclassifier2Prob.fit(x_trainSVM, y_train.ravel())\n",
    "\tclassifier3.fit(x_train, y_train.ravel())\n",
    "\tclassifier4.fit(x_train, y_train.ravel())\n",
    "\t# classifier5.fit(x_train, y_train.ravel())\n",
    "\tprint(\"Models Trained\")\n",
    "\n",
    "\ty_pred1 = classifier1.predict(x_test)\n",
    "\ty_pred2 = classifier2.predict(x_testSVM)\n",
    "\ty_pred3 = classifier3.predict(x_test)\n",
    "\ty_pred4 = classifier4.predict(x_test)\n",
    "\t# y_pred5 = classifier5.predict(x_test)\n",
    "\tprint(\"Models Tested - Normal\")\n",
    "\n",
    "\ty_proba_pred1 = classifier1.predict_proba(x_test)[:,1]\n",
    "\ty_proba_pred2 = classifier2Prob.predict_proba(x_testSVM)[:,1]\n",
    "\ty_proba_pred3 = classifier3.predict_proba(x_test)[:,1]\n",
    "\ty_proba_pred4 = classifier4.predict_proba(x_test)[:,1]\n",
    "\t# y_proba_pred5 = classifier5.predict_proba(x_test)[:,1]\n",
    "\tprint(\"Models Tested - Probability\")\n",
    "\n",
    "\tfor i in range(len(x_test)):\n",
    "\t\tfinal_preds1.append(y_pred1[i])\n",
    "\t\tfinal_preds2.append(y_pred2[i])\n",
    "\t\tfinal_preds3.append(y_pred3[i])\n",
    "\t\tfinal_preds4.append(y_pred4[i])\n",
    "\t\t# final_preds5.append(y_pred5[i])\n",
    "\n",
    "\t\tproba_preds1.append(y_proba_pred1[i])\n",
    "\t\tproba_preds2.append(y_proba_pred2[i])\n",
    "\t\tproba_preds3.append(y_proba_pred3[i])\n",
    "\t\tproba_preds4.append(y_proba_pred4[i])\n",
    "\t\t# proba_preds5.append(y_proba_pred5[i])\n",
    "\n",
    "\t\tfinal_truths.append(y_test[i])\n",
    "\n",
    "\n",
    "\n",
    "# creates ROC curve and calculates AUROC\n",
    "fpr1, tpr1, thresholds1 = metrics.roc_curve(final_truths, proba_preds1)\n",
    "fpr2, tpr2, thresholds1 = metrics.roc_curve(final_truths, proba_preds2)\n",
    "fpr3, tpr3, thresholds1 = metrics.roc_curve(final_truths, proba_preds3)\n",
    "fpr4, tpr4, thresholds1 = metrics.roc_curve(final_truths, proba_preds4)\n",
    "# fpr5, tpr5, thresholds1 = metrics.roc_curve(final_truths, proba_preds5)\n",
    "\n",
    "auc1 = metrics.roc_auc_score(final_truths, final_preds1)\n",
    "auc2 = metrics.roc_auc_score(final_truths, final_preds2)\n",
    "auc3 = metrics.roc_auc_score(final_truths, final_preds3)\n",
    "auc4 = metrics.roc_auc_score(final_truths, final_preds4)\n",
    "# auc5 = metrics.roc_auc_score(final_truths, final_preds5)\n",
    "\n",
    "plt.plot(fpr1, tpr1, label = '%s (area = %0.2f)' % ('Random Forest', auc1))\n",
    "plt.plot(fpr2, tpr2, label = '%s (area = %0.2f)' % ('Support Vector Machine', auc2))\n",
    "plt.plot(fpr3, tpr3, label = '%s (area = %0.2f)' % ('Logistic Regression', auc3))\n",
    "plt.plot(fpr4, tpr4, label = '%s (area = %0.2f)' % ('K-Nearest Neighbors', auc4))\n",
    "# plt.plot(fpr5, tpr5, label = '%s ROC (area = %0.2f)' % ('Multi-Layer Perceptron', auc5))\n",
    "\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.0])\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)')\n",
    "plt.ylabel('True Positive Rate (Sensitivity)')\n",
    "plt.title('Receiver Operating Characteristic Curves for Various Models')\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (60, 15), dpi = 400)\n",
    "# tree.plot_tree(classifier.estimators_[0], feature_names = labels, class_names = classes, filled = True, impurity = False, precision = 1, fontsize = 10)\n",
    "# fig.savefig('decisionTree.png')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load the data\n",
    "dataset = pd.read_csv(r'C:\\Users\\jeffe\\Downloads\\face_det_hardware\\HumanDetection-Kinect-Mmwave\\model\\ThermalOcc.csv')\n",
    "dataset = dataset.values  # Convert to numpy array\n",
    "target_data = dataset[:, 5]  # Assuming the target is in the 6th column (index 5)\n",
    "num_entries = len(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'none'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 61\u001b[0m\n\u001b[0;32m     56\u001b[0m y_train, y_test \u001b[38;5;241m=\u001b[39m target_data[train_index], target_data[test_index]\n\u001b[0;32m     58\u001b[0m classifier \u001b[38;5;241m=\u001b[39m RandomForestClassifier(max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m6\u001b[39m, min_samples_leaf\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, min_samples_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, \n\u001b[0;32m     59\u001b[0m                                     class_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m'\u001b[39m, bootstrap\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[0;32m     60\u001b[0m                                     max_features\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msqrt\u001b[39m\u001b[38;5;124m\"\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m classifier\u001b[38;5;241m.\u001b[39mfit(x_train, y_train)\n\u001b[0;32m     62\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m classifier\u001b[38;5;241m.\u001b[39mpredict(x_test)\n\u001b[0;32m     64\u001b[0m face_preds\u001b[38;5;241m.\u001b[39mextend(y_pred)\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m fit_method(estimator, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:363\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 363\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_data(\n\u001b[0;32m    364\u001b[0m     X,\n\u001b[0;32m    365\u001b[0m     y,\n\u001b[0;32m    366\u001b[0m     multi_output\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m    367\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcsc\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    368\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mDTYPE,\n\u001b[0;32m    369\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    370\u001b[0m )\n\u001b[0;32m    371\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[0;32m    372\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[0;32m    373\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[0;32m    374\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[0;32m    375\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\sklearn\\base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[0;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[0;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m check_X_y(X, y, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params)\n\u001b[0;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[0;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py:1263\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[0;32m   1258\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[0;32m   1259\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1260\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1261\u001b[0m     )\n\u001b[1;32m-> 1263\u001b[0m X \u001b[38;5;241m=\u001b[39m check_array(\n\u001b[0;32m   1264\u001b[0m     X,\n\u001b[0;32m   1265\u001b[0m     accept_sparse\u001b[38;5;241m=\u001b[39maccept_sparse,\n\u001b[0;32m   1266\u001b[0m     accept_large_sparse\u001b[38;5;241m=\u001b[39maccept_large_sparse,\n\u001b[0;32m   1267\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m   1268\u001b[0m     order\u001b[38;5;241m=\u001b[39morder,\n\u001b[0;32m   1269\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m   1270\u001b[0m     force_all_finite\u001b[38;5;241m=\u001b[39mforce_all_finite,\n\u001b[0;32m   1271\u001b[0m     ensure_2d\u001b[38;5;241m=\u001b[39mensure_2d,\n\u001b[0;32m   1272\u001b[0m     allow_nd\u001b[38;5;241m=\u001b[39mallow_nd,\n\u001b[0;32m   1273\u001b[0m     ensure_min_samples\u001b[38;5;241m=\u001b[39mensure_min_samples,\n\u001b[0;32m   1274\u001b[0m     ensure_min_features\u001b[38;5;241m=\u001b[39mensure_min_features,\n\u001b[0;32m   1275\u001b[0m     estimator\u001b[38;5;241m=\u001b[39mestimator,\n\u001b[0;32m   1276\u001b[0m     input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1277\u001b[0m )\n\u001b[0;32m   1279\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[0;32m   1281\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\sklearn\\utils\\validation.py:997\u001b[0m, in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[0;32m    995\u001b[0m         array \u001b[38;5;241m=\u001b[39m xp\u001b[38;5;241m.\u001b[39mastype(array, dtype, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    996\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 997\u001b[0m         array \u001b[38;5;241m=\u001b[39m _asarray_with_order(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype, xp\u001b[38;5;241m=\u001b[39mxp)\n\u001b[0;32m    998\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ComplexWarning \u001b[38;5;28;01mas\u001b[39;00m complex_warning:\n\u001b[0;32m    999\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1000\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComplex data not supported\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(array)\n\u001b[0;32m   1001\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcomplex_warning\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\sklearn\\utils\\_array_api.py:521\u001b[0m, in \u001b[0;36m_asarray_with_order\u001b[1;34m(array, dtype, order, copy, xp)\u001b[0m\n\u001b[0;32m    519\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39marray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    520\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 521\u001b[0m     array \u001b[38;5;241m=\u001b[39m numpy\u001b[38;5;241m.\u001b[39masarray(array, order\u001b[38;5;241m=\u001b[39morder, dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[0;32m    523\u001b[0m \u001b[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001b[39;00m\n\u001b[0;32m    524\u001b[0m \u001b[38;5;66;03m# container that is consistent with the input's namespace.\u001b[39;00m\n\u001b[0;32m    525\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xp\u001b[38;5;241m.\u001b[39masarray(array)\n",
      "\u001b[1;31mValueError\u001b[0m: could not convert string to float: 'none'"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "\n",
    "# FUNCTIONS:\n",
    "def create_conf_matrix(y_preds, y_reals):\n",
    "    TP = FN = FP = TN = 0\n",
    "    falsePos = []\n",
    "    falseNeg = []\n",
    "    for i in range(len(y_preds)):\n",
    "        pred_val = y_preds[i]\n",
    "        test_val = y_reals[i]\n",
    "        if test_val == 1 and pred_val == 1:\n",
    "            TP += 1\n",
    "        elif test_val == 1 and pred_val == 0:\n",
    "            FN += 1\n",
    "            falseNeg.append(i)\n",
    "        elif test_val == 0 and pred_val == 1:\n",
    "            FP += 1\n",
    "            falsePos.append(i)\n",
    "        elif test_val == 0 and pred_val == 0:\n",
    "            TN += 1\n",
    "    falsePos.sort()\n",
    "    falseNeg.sort()\n",
    "    return [TP, FN, FP, TN]\n",
    "\n",
    "def accuracy(matrix):\n",
    "    return (matrix[0] + matrix[3]) / sum(matrix)\n",
    "\n",
    "def sensitivity(matrix):\n",
    "    return matrix[0] / (matrix[0] + matrix[1])\n",
    "\n",
    "def specificity(matrix):\n",
    "    return matrix[3] / (matrix[2] + matrix[3])\n",
    "\n",
    "# Load the data\n",
    "dataset = pd.read_csv(r'C:\\Users\\jeffe\\Downloads\\face_det_hardware\\HumanDetection-Kinect-Mmwave\\model\\ThermalOcc.csv')\n",
    "dataset = dataset.values  # Convert to numpy array\n",
    "target_data = dataset[:, 5]  # Assuming the target is in the 6th column (index 5)\n",
    "num_entries = len(dataset)\n",
    "\n",
    "labels = [\"ID\", \"CONF\"]  # Update these with your actual column names\n",
    "classes = [\"0\", \"1\"]\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=100)\n",
    "face_preds = []\n",
    "face_truths = []\n",
    "\n",
    "for train_index, test_index in kfold.split(dataset):\n",
    "    x_train, x_test = dataset[train_index], dataset[test_index]\n",
    "    y_train, y_test = target_data[train_index], target_data[test_index]\n",
    "    \n",
    "    classifier = RandomForestClassifier(max_depth=6, min_samples_leaf=3, min_samples_split=10, \n",
    "                                        class_weight='balanced', bootstrap=False, \n",
    "                                        max_features=\"sqrt\", random_state=100)\n",
    "    classifier.fit(x_train, y_train)\n",
    "    y_pred = classifier.predict(x_test)\n",
    "    \n",
    "    face_preds.extend(y_pred)\n",
    "    face_truths.extend(y_test)\n",
    "\n",
    "matrix = create_conf_matrix(face_preds, face_truths)\n",
    "print(\"[TP, FN, FP, TN] =\", matrix)\n",
    "print(\"\\nAccuracy:\", accuracy(matrix))\n",
    "print(\"Sensitivity:\", sensitivity(matrix))\n",
    "print(\"Specificity:\", specificity(matrix))\n",
    "\n",
    "# Uncomment these lines if you want to plot the decision tree\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(35, 10), dpi=400)\n",
    "# tree.plot_tree(classifier.estimators_[0], feature_names=labels, class_names=classes, \n",
    "#                filled=True, impurity=False, precision=1, fontsize=10)\n",
    "# fig.savefig('decisionTree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\sklearn\\impute\\_base.py:577: UserWarning: Skipping features without any observed values: ['Unnamed: 25']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (29, 27), indices imply (29, 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 53\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;66;03m# Handle missing values\u001b[39;00m\n\u001b[0;32m     52\u001b[0m imputer \u001b[38;5;241m=\u001b[39m SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 53\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(imputer\u001b[38;5;241m.\u001b[39mfit_transform(dataset), columns\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     55\u001b[0m \u001b[38;5;66;03m# Separate features and target\u001b[39;00m\n\u001b[0;32m     56\u001b[0m X \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mdrop(dataset\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m5\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Assuming target is in the 6th column\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\pandas\\core\\frame.py:827\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    816\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    817\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    818\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    824\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[0;32m    825\u001b[0m         )\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 827\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    828\u001b[0m             data,\n\u001b[0;32m    829\u001b[0m             index,\n\u001b[0;32m    830\u001b[0m             columns,\n\u001b[0;32m    831\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    832\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    833\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    834\u001b[0m         )\n\u001b[0;32m    836\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    334\u001b[0m )\n\u001b[1;32m--> 336\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (29, 27), indices imply (29, 28)"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# FUNCTIONS:\n",
    "def create_conf_matrix(y_preds, y_reals):\n",
    "    TP = FN = FP = TN = 0\n",
    "    falsePos = []\n",
    "    falseNeg = []\n",
    "    for i in range(len(y_preds)):\n",
    "        pred_val = y_preds[i]\n",
    "        test_val = y_reals[i]\n",
    "        if test_val == 1 and pred_val == 1:\n",
    "            TP += 1\n",
    "        elif test_val == 1 and pred_val == 0:\n",
    "            FN += 1\n",
    "            falseNeg.append(i)\n",
    "        elif test_val == 0 and pred_val == 1:\n",
    "            FP += 1\n",
    "            falsePos.append(i)\n",
    "        elif test_val == 0 and pred_val == 0:\n",
    "            TN += 1\n",
    "    falsePos.sort()\n",
    "    falseNeg.sort()\n",
    "    return [TP, FN, FP, TN]\n",
    "\n",
    "def accuracy(matrix):\n",
    "    return (matrix[0] + matrix[3]) / sum(matrix)\n",
    "\n",
    "def sensitivity(matrix):\n",
    "    return matrix[0] / (matrix[0] + matrix[1])\n",
    "\n",
    "def specificity(matrix):\n",
    "    return matrix[3] / (matrix[2] + matrix[3])\n",
    "\n",
    "# Load the data\n",
    "dataset = pd.read_csv(r'C:\\Users\\jeffe\\Downloads\\face_det_hardware\\HumanDetection-Kinect-Mmwave\\model\\ThermalOcc.csv')\n",
    "# Preprocess the data\n",
    "le = LabelEncoder()\n",
    "for column in dataset.columns:\n",
    "    if dataset[column].dtype == 'object':\n",
    "        dataset[column] = le.fit_transform(dataset[column].astype(str))\n",
    "\n",
    "# Handle missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "dataset = pd.DataFrame(imputer.fit_transform(dataset), columns=dataset.columns)\n",
    "\n",
    "# Separate features and target\n",
    "X = dataset.drop(dataset.columns[5], axis=1)  # Assuming target is in the 6th column\n",
    "y = dataset.iloc[:, 5]\n",
    "\n",
    "num_entries = len(dataset)\n",
    "\n",
    "labels = X.columns.tolist()\n",
    "classes = [\"0\", \"1\"]\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=100)\n",
    "face_preds = []\n",
    "face_truths = []\n",
    "\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    classifier = RandomForestClassifier(max_depth=6, min_samples_leaf=3, min_samples_split=10, \n",
    "                                        class_weight='balanced', bootstrap=False, \n",
    "                                        max_features=\"sqrt\", random_state=100)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    face_preds.extend(y_pred)\n",
    "    face_truths.extend(y_test)\n",
    "\n",
    "matrix = create_conf_matrix(face_preds, face_truths)\n",
    "print(\"[TP, FN, FP, TN] =\", matrix)\n",
    "print(\"\\nAccuracy:\", accuracy(matrix))\n",
    "print(\"Sensitivity:\", sensitivity(matrix))\n",
    "print(\"Specificity:\", specificity(matrix))\n",
    "\n",
    "# Uncomment these lines if you want to plot the decision tree\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(35, 10), dpi=400)\n",
    "# tree.plot_tree(classifier.estimators_[0], feature_names=labels, class_names=classes, \n",
    "#                filled=True, impurity=False, precision=1, fontsize=10)\n",
    "# fig.savefig('decisionTree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29 entries, 0 to 28\n",
      "Data columns (total 28 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Distance (1 ft)  14 non-null     object \n",
      " 1   AE               29 non-null     float64\n",
      " 2   AE (NG)          29 non-null     float64\n",
      " 3   AJ               29 non-null     float64\n",
      " 4   AM (KP)          15 non-null     float64\n",
      " 5   AM (KP) (NG)     15 non-null     float64\n",
      " 6   AP               29 non-null     float64\n",
      " 7   ARN              29 non-null     float64\n",
      " 8   AX               15 non-null     float64\n",
      " 9   AX (NG)          29 non-null     float64\n",
      " 10  HY               29 non-null     float64\n",
      " 11  JL               29 non-null     float64\n",
      " 12  JL (NG)          29 non-null     float64\n",
      " 13  JY               29 non-null     object \n",
      " 14  JY (NG)          29 non-null     float64\n",
      " 15  LD               29 non-null     float64\n",
      " 16  LF               29 non-null     float64\n",
      " 17  OA               29 non-null     float64\n",
      " 18  RD               29 non-null     float64\n",
      " 19  RD (NG)          29 non-null     float64\n",
      " 20  SM               29 non-null     float64\n",
      " 21  SS               29 non-null     float64\n",
      " 22  SS (NG)          29 non-null     float64\n",
      " 23  SZ               29 non-null     float64\n",
      " 24  TP               29 non-null     float64\n",
      " 25  Unnamed: 25      0 non-null      float64\n",
      " 26  Average          29 non-null     float64\n",
      " 27  Unnamed: 27      29 non-null     int64  \n",
      "dtypes: float64(25), int64(1), object(2)\n",
      "memory usage: 6.5+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\sklearn\\impute\\_base.py:577: UserWarning: Skipping features without any observed values: ['Unnamed: 25']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (29, 27), indices imply (29, 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[33], line 59\u001b[0m\n\u001b[0;32m     57\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mapply(pd\u001b[38;5;241m.\u001b[39mto_numeric, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Convert all to numeric, invalid to NaN\u001b[39;00m\n\u001b[0;32m     58\u001b[0m imputer \u001b[38;5;241m=\u001b[39m SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 59\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(imputer\u001b[38;5;241m.\u001b[39mfit_transform(dataset), columns\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     61\u001b[0m \u001b[38;5;66;03m# Separate features and target\u001b[39;00m\n\u001b[0;32m     62\u001b[0m X \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mdrop(dataset\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m5\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Assuming target is in the 6th column\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\pandas\\core\\frame.py:827\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    816\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    817\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    818\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    824\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[0;32m    825\u001b[0m         )\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 827\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    828\u001b[0m             data,\n\u001b[0;32m    829\u001b[0m             index,\n\u001b[0;32m    830\u001b[0m             columns,\n\u001b[0;32m    831\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    832\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    833\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    834\u001b[0m         )\n\u001b[0;32m    836\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    334\u001b[0m )\n\u001b[1;32m--> 336\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (29, 27), indices imply (29, 28)"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import tree\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# FUNCTIONS:\n",
    "def create_conf_matrix(y_preds, y_reals):\n",
    "    TP = FN = FP = TN = 0\n",
    "    falsePos = []\n",
    "    falseNeg = []\n",
    "    for i in range(len(y_preds)):\n",
    "        pred_val = y_preds[i]\n",
    "        test_val = y_reals[i]\n",
    "        if test_val == 1 and pred_val == 1:\n",
    "            TP += 1\n",
    "        elif test_val == 1 and pred_val == 0:\n",
    "            FN += 1\n",
    "            falseNeg.append(i)\n",
    "        elif test_val == 0 and pred_val == 1:\n",
    "            FP += 1\n",
    "            falsePos.append(i)\n",
    "        elif test_val == 0 and pred_val == 0:\n",
    "            TN += 1\n",
    "    falsePos.sort()\n",
    "    falseNeg.sort()\n",
    "    return [TP, FN, FP, TN]\n",
    "\n",
    "def accuracy(matrix):\n",
    "    return (matrix[0] + matrix[3]) / sum(matrix)\n",
    "\n",
    "def sensitivity(matrix):\n",
    "    return matrix[0] / (matrix[0] + matrix[1])\n",
    "\n",
    "def specificity(matrix):\n",
    "    return matrix[3] / (matrix[2] + matrix[3])\n",
    "\n",
    "# Load the data\n",
    "dataset = pd.read_csv(r'C:\\Users\\jeffe\\Downloads\\face_det_hardware\\HumanDetection-Kinect-Mmwave\\model\\ThermalOcc.csv')\n",
    "\n",
    "# Print dataset info\n",
    "print(dataset.info())\n",
    "\n",
    "# Preprocess the data\n",
    "le = LabelEncoder()\n",
    "for column in dataset.columns:\n",
    "    if dataset[column].dtype == 'object':\n",
    "        dataset[column] = dataset[column].fillna('unknown')  # Fill NaN with 'unknown'\n",
    "        dataset[column] = le.fit_transform(dataset[column].astype(str))\n",
    "\n",
    "# Handle missing values\n",
    "dataset = dataset.apply(pd.to_numeric, errors='coerce')  # Convert all to numeric, invalid to NaN\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "dataset = pd.DataFrame(imputer.fit_transform(dataset), columns=dataset.columns)\n",
    "\n",
    "# Separate features and target\n",
    "X = dataset.drop(dataset.columns[5], axis=1)  # Assuming target is in the 6th column\n",
    "y = dataset.iloc[:, 5]\n",
    "\n",
    "num_entries = len(dataset)\n",
    "\n",
    "labels = X.columns.tolist()\n",
    "classes = [\"0\", \"1\"]\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=100)\n",
    "face_preds = []\n",
    "face_truths = []\n",
    "\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "    y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "    \n",
    "    classifier = RandomForestClassifier(max_depth=6, min_samples_leaf=3, min_samples_split=10, \n",
    "                                        class_weight='balanced', bootstrap=False, \n",
    "                                        max_features=\"sqrt\", random_state=100)\n",
    "    classifier.fit(X_train, y_train)\n",
    "    y_pred = classifier.predict(X_test)\n",
    "    \n",
    "    face_preds.extend(y_pred)\n",
    "    face_truths.extend(y_test)\n",
    "\n",
    "matrix = create_conf_matrix(face_preds, face_truths)\n",
    "print(\"[TP, FN, FP, TN] =\", matrix)\n",
    "print(\"\\nAccuracy:\", accuracy(matrix))\n",
    "print(\"Sensitivity:\", sensitivity(matrix))\n",
    "print(\"Specificity:\", specificity(matrix))\n",
    "\n",
    "# Uncomment these lines if you want to plot the decision tree\n",
    "# fig, axes = plt.subplots(nrows=1, ncols=1, figsize=(35, 10), dpi=400)\n",
    "# tree.plot_tree(classifier.estimators_[0], feature_names=labels, class_names=classes, \n",
    "#                filled=True, impurity=False, precision=1, fontsize=10)\n",
    "# fig.savefig('decisionTree.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 29 entries, 0 to 28\n",
      "Data columns (total 28 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   Distance (1 ft)  14 non-null     object \n",
      " 1   AE               29 non-null     float64\n",
      " 2   AE (NG)          29 non-null     float64\n",
      " 3   AJ               29 non-null     float64\n",
      " 4   AM (KP)          15 non-null     float64\n",
      " 5   AM (KP) (NG)     15 non-null     float64\n",
      " 6   AP               29 non-null     float64\n",
      " 7   ARN              29 non-null     float64\n",
      " 8   AX               15 non-null     float64\n",
      " 9   AX (NG)          29 non-null     float64\n",
      " 10  HY               29 non-null     float64\n",
      " 11  JL               29 non-null     float64\n",
      " 12  JL (NG)          29 non-null     float64\n",
      " 13  JY               29 non-null     object \n",
      " 14  JY (NG)          29 non-null     float64\n",
      " 15  LD               29 non-null     float64\n",
      " 16  LF               29 non-null     float64\n",
      " 17  OA               29 non-null     float64\n",
      " 18  RD               29 non-null     float64\n",
      " 19  RD (NG)          29 non-null     float64\n",
      " 20  SM               29 non-null     float64\n",
      " 21  SS               29 non-null     float64\n",
      " 22  SS (NG)          29 non-null     float64\n",
      " 23  SZ               29 non-null     float64\n",
      " 24  TP               29 non-null     float64\n",
      " 25  Unnamed: 25      0 non-null      float64\n",
      " 26  Average          29 non-null     float64\n",
      " 27  Unnamed: 27      29 non-null     int64  \n",
      "dtypes: float64(25), int64(1), object(2)\n",
      "memory usage: 6.5+ KB\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\sklearn\\impute\\_base.py:577: UserWarning: Skipping features without any observed values: ['Unnamed: 25']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Shape of passed values is (29, 27), indices imply (29, 28)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 57\u001b[0m\n\u001b[0;32m     55\u001b[0m dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mapply(pd\u001b[38;5;241m.\u001b[39mto_numeric, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoerce\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# Convert all to numeric, invalid to NaN\u001b[39;00m\n\u001b[0;32m     56\u001b[0m imputer \u001b[38;5;241m=\u001b[39m SimpleImputer(strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 57\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(imputer\u001b[38;5;241m.\u001b[39mfit_transform(dataset), columns\u001b[38;5;241m=\u001b[39mdataset\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m     59\u001b[0m \u001b[38;5;66;03m# Separate features and target\u001b[39;00m\n\u001b[0;32m     60\u001b[0m X \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mdrop(dataset\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m4\u001b[39m], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Assuming target is in the 6th column\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\pandas\\core\\frame.py:827\u001b[0m, in \u001b[0;36mDataFrame.__init__\u001b[1;34m(self, data, index, columns, dtype, copy)\u001b[0m\n\u001b[0;32m    816\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m dict_to_mgr(\n\u001b[0;32m    817\u001b[0m             \u001b[38;5;66;03m# error: Item \"ndarray\" of \"Union[ndarray, Series, Index]\" has no\u001b[39;00m\n\u001b[0;32m    818\u001b[0m             \u001b[38;5;66;03m# attribute \"name\"\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    824\u001b[0m             copy\u001b[38;5;241m=\u001b[39m_copy,\n\u001b[0;32m    825\u001b[0m         )\n\u001b[0;32m    826\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 827\u001b[0m         mgr \u001b[38;5;241m=\u001b[39m ndarray_to_mgr(\n\u001b[0;32m    828\u001b[0m             data,\n\u001b[0;32m    829\u001b[0m             index,\n\u001b[0;32m    830\u001b[0m             columns,\n\u001b[0;32m    831\u001b[0m             dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    832\u001b[0m             copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[0;32m    833\u001b[0m             typ\u001b[38;5;241m=\u001b[39mmanager,\n\u001b[0;32m    834\u001b[0m         )\n\u001b[0;32m    836\u001b[0m \u001b[38;5;66;03m# For data is list-like, or Iterable (will consume into list)\u001b[39;00m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_list_like(data):\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:336\u001b[0m, in \u001b[0;36mndarray_to_mgr\u001b[1;34m(values, index, columns, dtype, copy, typ)\u001b[0m\n\u001b[0;32m    331\u001b[0m \u001b[38;5;66;03m# _prep_ndarraylike ensures that values.ndim == 2 at this point\u001b[39;00m\n\u001b[0;32m    332\u001b[0m index, columns \u001b[38;5;241m=\u001b[39m _get_axes(\n\u001b[0;32m    333\u001b[0m     values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], values\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns\n\u001b[0;32m    334\u001b[0m )\n\u001b[1;32m--> 336\u001b[0m _check_values_indices_shape_match(values, index, columns)\n\u001b[0;32m    338\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m typ \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marray\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    339\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[1;32mc:\\Users\\jeffe\\anaconda\\Lib\\site-packages\\pandas\\core\\internals\\construction.py:420\u001b[0m, in \u001b[0;36m_check_values_indices_shape_match\u001b[1;34m(values, index, columns)\u001b[0m\n\u001b[0;32m    418\u001b[0m passed \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mshape\n\u001b[0;32m    419\u001b[0m implied \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mlen\u001b[39m(index), \u001b[38;5;28mlen\u001b[39m(columns))\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShape of passed values is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpassed\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, indices imply \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimplied\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Shape of passed values is (29, 27), indices imply (29, 28)"
     ]
    }
   ],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import KFold\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# FUNCTIONS:\n",
    "def create_conf_matrix(y_preds, y_reals):\n",
    "    TP = FN = FP = TN = 0\n",
    "    falsePos = []\n",
    "    falseNeg = []\n",
    "    for i in range(len(y_preds)):\n",
    "        pred_val = y_preds[i]\n",
    "        test_val = y_reals[i]\n",
    "        if test_val == 1 and pred_val == 1:\n",
    "            TP += 1\n",
    "        elif test_val == 1 and pred_val == 0:\n",
    "            FN += 1\n",
    "            falseNeg.append(i)\n",
    "        elif test_val == 0 and pred_val == 1:\n",
    "            FP += 1\n",
    "            falsePos.append(i)\n",
    "        elif test_val == 0 and pred_val == 0:\n",
    "            TN += 1\n",
    "    falsePos.sort()\n",
    "    falseNeg.sort()\n",
    "    return [TP, FN, FP, TN]\n",
    "\n",
    "def accuracy(matrix):\n",
    "    return (matrix[0] + matrix[3]) / sum(matrix)\n",
    "\n",
    "def sensitivity(matrix):\n",
    "    return matrix[0] / (matrix[0] + matrix[1])\n",
    "\n",
    "def specificity(matrix):\n",
    "    return matrix[3] / (matrix[2] + matrix[3])\n",
    "\n",
    "# Load the data\n",
    "dataset = pd.read_csv(r'C:\\Users\\jeffe\\Downloads\\face_det_hardware\\HumanDetection-Kinect-Mmwave\\model\\ThermalOcc.csv')\n",
    "\n",
    "# Print dataset info\n",
    "print(dataset.info())\n",
    "\n",
    "# Preprocess the data\n",
    "le = LabelEncoder()\n",
    "for column in dataset.columns:\n",
    "    if dataset[column].dtype == 'object':\n",
    "        dataset[column] = dataset[column].fillna('unknown')  # Fill NaN with 'unknown'\n",
    "        dataset[column] = le.fit_transform(dataset[column].astype(str))\n",
    "\n",
    "# Handle missing values\n",
    "dataset = dataset.apply(pd.to_numeric, errors='coerce')  # Convert all to numeric, invalid to NaN\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "dataset = pd.DataFrame(imputer.fit_transform(dataset), columns=dataset.columns)\n",
    "\n",
    "# Separate features and target\n",
    "X = dataset.drop(dataset.columns[4], axis=1)  # Assuming target is in the 6th column\n",
    "y = dataset.iloc[:, 4]\n",
    "\n",
    "num_entries = len(dataset)\n",
    "\n",
    "labels = X.columns.tolist()\n",
    "classes = [\"0\", \"1\"]\n",
    "\n",
    "kfold = KFold(n_splits=10, shuffle=True, random_state=100)\n",
    "face_preds = []\n",
    "face_truths = []\n",
    "\n",
    "for train_index, test_index in kfold.split(X):\n",
    "    X_train, X_test = X\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
